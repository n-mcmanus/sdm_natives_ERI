---
title: "Filter species occurrence data"
author: "Nick McManus"
date: "2023-07-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)            ## always
library(here)                 ## reading/writing data
library(rgbif)                ## download GBIF data
library(CoordinateCleaner)    ## filter/clean GBIF data
library(terra)
library(raster)
library(enmSdmX)
library(spThin)
library(dismo)
```

## Download Occurrence Data

### CalFlora
This data was directly downloaded from the website as separate CSVs for each species. We'll read them all in as one dataframe to more easily filter the data. 
*NOTE:* Creating a "low" and "high" filtered version of the data to see how much of a difference strictness over data sources plays. 
```{r}
files_cf <- list.files(path = here("data/occ/calflora/"),
                       full.names = TRUE)

cfOcc <- data.frame()

## combine all spp into one df
for (i in 1:length(files_cf)){
  df <- read_csv(files_cf[i])
  
  cfOcc <- rbind(cfOcc, df)
}

## List of accepted sources
sources <- paste(c("BLM",
                   "Bureau",
                   "USDA",
                   "DFW",
                   "USGS",
                   "Nature Conservancy",
                   "TNC",
                   "CNPS",
                   "Taylor",
                   "Hrusa"), collapse = "|")

## Low filter
calfOcc_low <- cfOcc %>% 
  ## GPS accuracy greater than raster cell size
  filter(`Location Quality` %in% c("high", "medium"),
         `Accuracy: Square Meters` <= 270)
write_csv(calfOcc_low, here("data/occ/calflora/combined_lowfilter.csv"))


## High filter
calfOcc_high <- calfOcc_low %>% 
  filter(str_detect(.$Source, sources))
write_csv(calfOcc_high, here("data/occ/calflora/combined_highfilter.csv"))
  
```


### GBIF

This script uses the `rgbif` package to directly download species occurence data from GBIF. To use the `occ_download()` function, this requires setting your GBIF login details. Details on how to set this can be found at: https://docs.ropensci.org/rgbif/articles/gbif_credentials.html
```{r}
## Pull taxon keys from list of spp
taxon_keys <- name_backbone_checklist(c(
  ## shrubs
  "Atriplex polycarpa",
  "Peritoma arborea",
  ## forbs
  "Centromadia pungens",
  "Layia pentachaeta subsp. albida D.D.Keck", 
  "Phacelia ciliata Benth.",
  ## spp that naturally colonized
  "Amsinckia menziesii (Lehm.) A.Nelson & J.F.Macbr.",
  "Caulanthus lasiophyllus (Hook. & Arn.) Payson"
)) %>% 
  pull(usageKey)

## var names
names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)


## Loop through spp list and save each
for (i in 1:length(taxon_keys)) {
  ### download filtered data for A. polycarpa
gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[i]),
                       ## remove geospatial issues
                       pred("hasGeospatialIssue", FALSE),
                       ## ensure coords
                       pred("hasCoordinate", TRUE),
                       ## remove "absent" occurences
                       pred("occurrenceStatus", "PRESENT"),
                       ## within US
                       pred("country", "US"),
                       ## within CA
                       pred("stateProvince", "California"),
                       ## uncertainty less than 5000m
                       #pred_lt("coordinateUncertaintyInMeters",5000),
                       ## output as CSV
                       format = "SIMPLE_CSV")

### check on download status
occ_download_wait(gbif_download)

### import GBIF data into env and filter using CoordinateCleaner pkg
 species <- occ_download_get(gbif_download, 
                             path = here("data/occ/gbif/zips/"),
                             overwrite = TRUE) %>%
   occ_download_import() %>% 
   ## set lowercase column names to work with CC
   setNames(tolower(names(.))) %>% 
   ## filter out duplicate points
   distinct(decimallongitude, decimallatitude, 
            specieskey, datasetkey, .keep_all = TRUE) %>% 
   ## filter known uncertainty below 270 and keep NAs
   filter(coordinateuncertaintyinmeters < 270 | is.na(coordinateuncertaintyinmeters)) %>% 
   ## known inaccurate default values
   filter(!coordinateuncertaintyinmeters %in% c(301,3036,999,9999)) %>% 
   ## remove herbaria/zoo locations
   cc_inst(lon = "decimallongitude", lat = "decimallatitude",
           buffer = 270, value = "clean", verbose = TRUE) %>% 
   ## remove ocean values
   cc_sea(lon = "decimallongitude", lat = "decimallatitude")
 
 ## export file
 write_csv(species, paste0(here("data/occ/gbif//"),names[i],"_gbif.csv"))
 
 ## Rename as local variable
 assign(names[i], species)
}
```

Start with A. polycarpa
```{r}
### download filtered data for A. polycarpa
gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[2]),
                       ## remove geospatial issues
                       pred("hasGeospatialIssue", FALSE),
                       ## ensure coords
                       pred("hasCoordinate", TRUE),
                       ## remove "absent" occurences
                       pred("occurrenceStatus", "PRESENT"),
                       ## within US
                       pred("country", "US"),
                       ## within CA
                       pred("stateProvince", "California"),
                       ## uncertainty less than 5000m
                       #pred_lt("coordinateUncertaintyInMeters",5000),
                       ## output as CSV
                       format = "SIMPLE_CSV")

### check on download status
occ_download_wait(gbif_download)

### import GBIF data into env and filter using CoordinateCleaner pkg
p_arborea2 <- occ_download_get(gbif_download, 
                              path = here("data/occ/gbif/zips/"),
                              overwrite = TRUE) %>%
  occ_download_import() %>% 
  ## set lowercase column names to work with CC
  setNames(tolower(names(.))) %>% 
  ## filter out duplicate points
  distinct(decimallongitude, decimallatitude, specieskey, datasetkey, .keep_all = TRUE) %>% 
  ## filter known uncertainty below 270 and keep NAs
  filter(coordinateuncertaintyinmeters < 270 | is.na(coordinateuncertaintyinmeters)) %>% 
  ## known inaccurate default values
  filter(!coordinateuncertaintyinmeters %in% c(301,3036,999,9999)) %>% 
  ## remove herbaria/zoo locations
  cc_inst(lon = "decimallongitude", lat = "decimallatitude",
          buffer = 270, value = "clean", verbose = TRUE) 
  ## remove accidental ocean values
  cc_sea(lon = "decimallongitude", lat = "decimallatitude")
```

Pull in CalFlora data for A. polycarpa, merge with GBIF data, and determine how many duplicates there are
```{r}
apoly_calf <- read_csv("data/a_polycarpa_calflora.csv") %>% 
  janitor::clean_names()

## subset of calflora data
apoly_calf_sub <- apoly_calf %>% 
  dplyr::select(c(latitude, longitude, date)) %>% 
  #mutate(source = "calflora") %>% 
  mutate(latitude = round(latitude, 4))

## subset of gbif data
apoly_gbif_sub <- a_poly %>% 
  dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
  rename(latitude = decimallatitude, 
         longitude = decimallongitude,  
         date = eventdate) %>% 
  #mutate(source = "gbif") %>% 
  mutate(latitude = round(latitude, 4))

apoly_gbif_sub$id <- as.character(apoly_gbif_sub$id)
apoly_gbif_sub$date <- as.Date(apoly_gbif_sub$date)


test <- inner_join(apoly_calf_sub, apoly_gbif_sub)

## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
## that came in both databases

apoly_gbif2 <- apoly_gbif_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

apoly_calf2 <- apoly_calf_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

test2 <- inner_join(apoly_gbif2, apoly_calf2)

## only 5 matching














## merge
apoly_merge <- rbind(apoly_calf_sub, apoly_gbif_sub)

## round lat
apoly_merge_sub <- apoly_merge %>% 
  mutate(latitude = round(latitude, 4)) %>% 
  group_by(date)


```

```{r}
parb_calf <- read_csv(here("data/p_arborea_calflora.csv")) %>% 
  janitor::clean_names()

## subset of calflora data
parb_calf_sub <- parb_calf %>% 
  dplyr::select(c(latitude, longitude, date)) %>% 
  #mutate(source = "calflora") %>% 
  mutate(latitude = round(latitude, 4))

## subset of gbif data
parb_gbif_sub <- p_arborea %>% 
  dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
  rename(latitude = decimallatitude, 
         longitude = decimallongitude,  
         date = eventdate) %>% 
  #mutate(source = "gbif") %>% 
  mutate(latitude = round(latitude, 4))

parb_gbif_sub$id <- as.character(apoly_gbif_sub$id)
parb_gbif_sub$date <- as.Date(parb_gbif_sub$date)


test <- inner_join(parb_calf_sub, parb_gbif_sub)

## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
## that came in both databases

parb_gbif2 <- parb_gbif_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

parb_calf2 <- parb_calf_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

test2 <- inner_join(parb_gbif2, parb_calf2)

## only 5 matching
```



## Spatial and Temporal Thinning

BCMv8 data are available at 270m resolution. To avoid biasing the model to oversampled regions, only one occurrence per 270m pixel will be used for extraction. Additionally, we are only interested in species occurrences in the water years 2000-2022. We'll thin and filter the data to be used in the SDM model. 
```{r}
## Read in species occurrence data
sppOcc <- read_csv(here('data/species_occurrence_GBIF/a_polycarpa.csv'))
## Read in reference raster
aet2020 <- rast(here('data/bcmv8/2021_2022/aet2020dec.tif')) %>% 
  project(y = "WGS84")

## Testing enmSdmX -----------------------------------------

## thin data to one pt per raster cell (270m)
sppThin <- elimCellDuplicates(sppOcc, aet2020, longLat = c("decimallongitude", "decimallatitude")) %>% 
  dplyr::select(c("decimallongitude", "decimallatitude")) %>%
  rename(Longitude = decimallongitude,
         Latitude = decimallatitude)

## Now filter to water years of interest
sppThin_2000_2022 <- sppThin %>% 
  ## only keep vars of interest
  dplyr::select(gbifid, species, month, year, 
                decimallatitude, decimallongitude) %>% 
  ## Keep Oct 1999 through Sep 2022
  filter(year >= 2000 | (year == 1999 & month %in% c(10,11,12))) %>% 
  filter(year <= 2022) %>% 
  filter(!(year == 2022 & month %in% c(10, 11, 12)))


# ## Double-check no duplicate points!
# test <- sppThin_2000_2022 %>%
#   group_by(decimallatitude) %>%
#   group_by(decimallongitude) %>%
#   distinct(decimallatitude, decimallongitude, .keep_all = T)


## Save 
write_csv(sppThin_2000_2022,
          here('data/species_occurrence_GBIF/a_polycarpa_thinned.csv'))

## Testing spThin -----------------------------------------
# thinTest <- thin(
#   loc.data = sppOcc,
#   lat.col = "decimallatitude",
#   long.col = "decimallongitude",
#   spec.col = "species",
#   thin.par = 0.27,
#   reps = 1,
#   locs.thinned.list.return = T,
#   write.files = F
#   # out.dir = here('data/species_occurrence_GBIF/')
# 
# )
# thinned <- as.data.frame(thinTest)
# #
# diff <- setdiff(sppThin, thinned)
# diff_vect <- diff %>%
#   vect(geom = c("Longitude", "Latitude"), crs = "WGS84")
# writeVector(diff_vect, here('data/diff'))
# #
# #
# enmTest_vect <- sppThin %>%
#   vect(geom = c("decimallongitude", "decimallatitude"), crs = "WGS84")
# 
# thinTest_vect <- thinned %>%
#   vect(geom = c("Longitude", "Latitude"), crs = "WGS84")


# writeVector(enmTest_vect, here('data/enmTest'))
# writeVector(thinTest_vect, here('data/thinTest'))

```



## Generate background points
The sample with enmSdmX is faster and compatible with terra, but the dismo one has the option of inputting occurrence data and NOT sampling those points... which may be a useful feature. For now using the Dismo fxn. 
```{r}
### Set up data inputs ----------------------------------------------
## Commented out code for presence data...
# pres <- read_csv(here('data/species_occurrence_GBIF/a_polycarpa_thinned.csv')) %>% 
#   dplyr::select(decimallatitude, decimallongitude) %>% 
#   rename(x = decimallongitude,
#          y = decimallatitude)
# ## Getting weird error in dismo::randomPoints
# ## Have to explicitly extablish values as dbl in a df
# pres$y = as.numeric(pres$y)
# pres$x = as.numeric(pres$x)
# pres <- as.data.frame(pres)

## Read in reference raster and make RasterLayer obj
aet2020 <- rast(here('data/bcmv8/2021_2022/aet2020dec.tif')) %>% 
  project(y = "WGS84")
aet2020_r <- raster(aet2020)

### Create random background points --------------------------------------
## Random samples w/o replacement
## ensure pts aren't in same cell as presence data 
sampOcc <- randomPoints(mask = aet2020_r, 
                        n = 11040,  
                        prob = FALSE)
sampOcc <- as.data.frame(sampOcc)

### confirm no two points in same pixel
# test <- elimCellDuplicates(sampOcc, aet2020, longLat = c("x", "y"))

### assign 40 pts to each mo/yr
### first create df of mo/yr in wy format
dates <- data.frame(month = rep(1:12, each = 40),
                    year = rep(2000:2022, each = 480))
dates <- rbind(tail(dates, 120), head(dates, -120))
dates[1:120, 2] = (dates[121,2] - 1)

backSamp <- cbind(sampOcc, dates)

### export as csv
write_csv(backSamp, here('data/species_occurrence_GBIF/background_points.csv'))
```










Eventually the SDM will be done with Dismo, but for now I'm starting with Wallace. Although GBIF datasets can be directly queried in the Wallace interface, you cannot customize the data filters as easily as using `rgbif` and `CoordinateCleaner`. As such, we'll have some temporary code that exports occurrence data locally that can be uploaded to Wallace interface.

```{r}
write_csv(a_poly, here("data/species_occurrence_GBIF/a_polycarpa.csv"))
```













Older version of cleaning data when manually downloading/reading in:

```{r}
### Fxn to read in and clean information from CSV
sp_filter <- function(filename) {
  ## read in data
  raw <- read_csv(here(paste0("data/species_occurrence_GBIF/raw/", filename))) %>% 
    janitor::clean_names()
  
  ## filter data
  filtered <- raw %>% 
    ## only keep obs in CA
    filter(state_province == "California") %>% 
    ## remove data with no coords
    drop_na(decimal_latitude, decimal_longitude) %>% 
    ## remove points with incorrect coords
    filter(!str_detect(issue, "COUNTRY_COORDINATE_MISMATCH"))
  
  return(filtered)
}

### test with a. poly
a_polycarpa <- sp_filter("A_polycarpa.csv")


blah <- read_csv(here("data/species_occurrence_GBIF/raw/A_polycarpa.csv"))
```

