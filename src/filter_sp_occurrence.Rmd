---
title: "Filter species occurrence data"
author: "Nick McManus"
date: "2023-07-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)            ## always
library(here)                 ## reading/writing data
library(rgbif)                ## download GBIF data
library(CoordinateCleaner)    ## filter/clean GBIF data
library(terra)
library(raster)
library(enmSdmX)
library(spThin)
library(dismo)
```

## Download Occurrence Data

This script uses the `rgbif` package to directly download species occurence data from GBIF. To use the `occ_download()` function, this requires setting your GBIF login details. Details on how to set this can be found at: https://docs.ropensci.org/rgbif/articles/gbif_credentials.html

```{r}
### Create list of species names
name_list <- c(
  ## shrubs
  "Atriplex polycarpa",
  "Peritoma arborea",
  ## forbs
  "Centromadia pungens",
  "Layia pentachaeta subsp. albida D.D.Keck", 
  "Phacelia ciliata Benth.",
  ## spp that naturally colonized
  "Amsinckia menziesii (Lehm.) A.Nelson & J.F.Macbr.",
  "Caulanthus lasiophyllus (Hook. & Arn.) Payson"
)

### Check if they worked
# name_backbone_checklist(name_list)

### Pull taxon keys from list
taxon_keys <- name_backbone_checklist(name_list) %>% 
  pull(usageKey)
```

Start with A. polycarpa
```{r}
### download filtered data for A. polycarpa
gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[2]),
                       ## remove geospatial issues
                       pred("hasGeospatialIssue", FALSE),
                       ## ensure coords
                       pred("hasCoordinate", TRUE),
                       ## remove "absent" occurences
                       pred("occurrenceStatus", "PRESENT"),
                       ## within US
                       pred("country", "US"),
                       ## within CA
                       pred("stateProvince", "California"),
                       ## uncertainty less than 5000m
                       #pred_lt("coordinateUncertaintyInMeters",5000),
                       ## output as CSV
                       format = "SIMPLE_CSV")

### check on download status
occ_download_wait(gbif_download)

### import GBIF data into env and filter using CoordinateCleaner pkg
p_arborea <- occ_download_get(gbif_download) %>%
  occ_download_import() %>% 
  ## set lowercase column names to work with CC
  setNames(tolower(names(.))) %>% 
  ## filter out duplicate points
  distinct(decimallongitude, decimallatitude, specieskey, datasetkey, .keep_all = TRUE)
```

Pull in CalFlora data for A. polycarpa, merge with GBIF data, and determine how many duplicates there are
```{r}
apoly_calf <- read_csv("data/a_polycarpa_calflora.csv") %>% 
  janitor::clean_names()

## subset of calflora data
apoly_calf_sub <- apoly_calf %>% 
  dplyr::select(c(latitude, longitude, date)) %>% 
  #mutate(source = "calflora") %>% 
  mutate(latitude = round(latitude, 4))

## subset of gbif data
apoly_gbif_sub <- a_poly %>% 
  dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
  rename(latitude = decimallatitude, 
         longitude = decimallongitude,  
         date = eventdate) %>% 
  #mutate(source = "gbif") %>% 
  mutate(latitude = round(latitude, 4))

apoly_gbif_sub$id <- as.character(apoly_gbif_sub$id)
apoly_gbif_sub$date <- as.Date(apoly_gbif_sub$date)


test <- inner_join(apoly_calf_sub, apoly_gbif_sub)

## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
## that came in both databases

apoly_gbif2 <- apoly_gbif_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

apoly_calf2 <- apoly_calf_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

test2 <- inner_join(apoly_gbif2, apoly_calf2)

## only 5 matching














## merge
apoly_merge <- rbind(apoly_calf_sub, apoly_gbif_sub)

## round lat
apoly_merge_sub <- apoly_merge %>% 
  mutate(latitude = round(latitude, 4)) %>% 
  group_by(date)


```

```{r}
parb_calf <- read_csv(here("data/p_arborea_calflora.csv")) %>% 
  janitor::clean_names()

## subset of calflora data
parb_calf_sub <- parb_calf %>% 
  dplyr::select(c(latitude, longitude, date)) %>% 
  #mutate(source = "calflora") %>% 
  mutate(latitude = round(latitude, 4))

## subset of gbif data
parb_gbif_sub <- p_arborea %>% 
  dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
  rename(latitude = decimallatitude, 
         longitude = decimallongitude,  
         date = eventdate) %>% 
  #mutate(source = "gbif") %>% 
  mutate(latitude = round(latitude, 4))

parb_gbif_sub$id <- as.character(apoly_gbif_sub$id)
parb_gbif_sub$date <- as.Date(parb_gbif_sub$date)


test <- inner_join(parb_calf_sub, parb_gbif_sub)

## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
## that came in both databases

parb_gbif2 <- parb_gbif_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

parb_calf2 <- parb_calf_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

test2 <- inner_join(parb_gbif2, parb_calf2)

## only 5 matching
```



## Spatial and Temporal Thinning

BCMv8 data are available at 270m resolution. To avoid biasing the model to oversampled regions, only one occurrence per 270m pixel will be used for extraction. Additionally, we are only interested in species occurrences in the water years 2000-2022. We'll thin and filter the data to be used in the SDM model. 
```{r}
## Read in species occurrence data
sppOcc <- read_csv(here('data/species_occurrence_GBIF/a_polycarpa.csv'))
## Read in reference raster
aet2020 <- rast(here('data/bcmv8/2021_2022/aet2020dec.tif')) %>% 
  project(y = "WGS84")

## Testing enmSdmX -----------------------------------------

## thin data to one pt per raster cell (270m)
sppThin <- elimCellDuplicates(sppOcc, aet2020, longLat = c("decimallongitude", "decimallatitude"))
  # dplyr::select(c("decimallongitude", "decimallatitude")) %>% 
  # rename(Longitude = decimallongitude,
  #        Latitude = decimallatitude)

## Now filter to water years of interest
sppThin_2000_2022 <- sppThin %>% 
  ## only keep vars of interest
  dplyr::select(gbifid, species, month, year, 
                decimallatitude, decimallongitude) %>% 
  ## Keep Oct 1999 through Sep 2022
  filter(year >= 2000 | (year == 1999 & month %in% c(10,11,12))) %>% 
  filter(year <= 2022) %>% 
  filter(!(year == 2022 & month %in% c(10, 11, 12)))


# ## Double-check no duplicate points!
# test <- sppThin_2000_2022 %>%
#   group_by(decimallatitude) %>%
#   group_by(decimallongitude) %>%
#   distinct(decimallatitude, decimallongitude, .keep_all = T)


## Save 
write_csv(sppThin_2000_2022,
          here('data/species_occurrence_GBIF/a_polycarpa_thinned.csv'))

## Testing spThin -----------------------------------------
# thinTest <- thin(
#   loc.data = sppOcc,
#   lat.col = "decimallatitude",
#   long.col = "decimallongitude",
#   spec.col = "species",
#   thin.par = 0.27,
#   reps = 1,
#   locs.thinned.list.return = F,
#   write.files = T,
#   out.dir = here('data/species_occurrence_GBIF/')
#   
# )
# thinned <- as.data.frame(thinTest)
# 
# diff <- setdiff(enmTest, thinTest)
# diff_vect <- diff %>% 
#   vect(geom = c("Longitude", "Latitude"), crs = "WGS84")
# writeVector(diff_vect, here('data/diff'))
# 
# 
# enmTest_vect <- enmTest %>%
#   vect(geom = c("decimallongitude", "decimallatitude"), crs = "WGS84")
# 
# thinTest_vect <- thinTest %>% 
#   vect(geom = c("Longitude", "Latitude"), crs = "WGS84")


# writeVector(enmTest_vect, here('data/enmTest'))
# writeVector(thinTest_vect, here('data/thinTest'))

```



## Generate background points
The sample with enmSdmX is faster and compatible with terra, but the dismo one has the option of inputting occurrence data and NOT sampling those points... which may be a useful feature. For now using the Dismo fxn. 
```{r}
### Set up data inputs ----------------------------------------------
## Read in presence data, only keep coords
pres <- read_csv(here('data/species_occurrence_GBIF/a_polycarpa_thinned.csv')) %>% 
  dplyr::select(decimallatitude, decimallongitude) %>% 
  rename(x = decimallongitude,
         y = decimallatitude)

## Getting weird error in dismo::randomPoints
## Have to explicitly extablish values as dbl in a df
pres$y = as.numeric(pres$y)
pres$x = as.numeric(pres$x)
pres <- as.data.frame(pres)

## Read in reference raster
aet2020_r <- raster(here('data/bcmv8/2021_2022/aet2020dec.tif')) 
raster::crs(aet2020_r) <- "EPSG: 4326"

### Create random background points --------------------------------------
## Random samples w/o replacement
## ensure pts aren't in same cell as presence data 
sampOcc <- randomPoints(mask = aet2020_r, 
                        p = pres,
                        n = 11040,  
                        prob = FALSE)
sampOcc <- as.data.frame(sampOcc)

### confirm no two points in same pixel
# test <- elimCellDuplicates(sampOcc, aet2020, longLat = c("x", "y"))

### assign 40 pts to each mo/yr
### first create df of mo/yr in wy format
dates <- data.frame(month = rep(1:12, each = 40),
                    year = rep(2000:2022, each = 480))
dates <- rbind(tail(dates, 120), head(dates, -120))
dates[1:120, 2] = (dates[121,2] - 1)

backSamp <- cbind(sampOcc, dates)

### export as csv
write_csv(backSamp, here('data/species_occurrence_GBIF/background_points.csv'))
```










Eventually the SDM will be done with Dismo, but for now I'm starting with Wallace. Although GBIF datasets can be directly queried in the Wallace interface, you cannot customize the data filters as easily as using `rgbif` and `CoordinateCleaner`. As such, we'll have some temporary code that exports occurrence data locally that can be uploaded to Wallace interface.

```{r}
write_csv(a_poly, here("data/species_occurrence_GBIF/a_polycarpa.csv"))
```













Older version of cleaning data when manually downloading/reading in:

```{r}
### Fxn to read in and clean information from CSV
sp_filter <- function(filename) {
  ## read in data
  raw <- read_csv(here(paste0("data/species_occurrence_GBIF/raw/", filename))) %>% 
    janitor::clean_names()
  
  ## filter data
  filtered <- raw %>% 
    ## only keep obs in CA
    filter(state_province == "California") %>% 
    ## remove data with no coords
    drop_na(decimal_latitude, decimal_longitude) %>% 
    ## remove points with incorrect coords
    filter(!str_detect(issue, "COUNTRY_COORDINATE_MISMATCH"))
  
  return(filtered)
}

### test with a. poly
a_polycarpa <- sp_filter("A_polycarpa.csv")


blah <- read_csv(here("data/species_occurrence_GBIF/raw/A_polycarpa.csv"))
```

