---
title: "Filter species occurrence data"
author: "Nick McManus"
date: "2023-07-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)            ## always
library(here)                 ## reading/writing data
library(rgbif)                ## download GBIF data
library(CoordinateCleaner)    ## filter/clean GBIF data
library(terra)
```

## R Markdown

This script uses the `rgbif` package to directly download species occurence data from GBIF. To use the `occ_download()` function, this requires setting your GBIF login details. Details on how to set this can be found at: https://docs.ropensci.org/rgbif/articles/gbif_credentials.html

```{r}
### Create list of species names
name_list <- c(
  ## shrubs
  "Atriplex polycarpa",
  "Peritoma arborea",
  ## forbs
  "Centromadia pungens",
  "Layia pentachaeta subsp. albida D.D.Keck", 
  "Phacelia ciliata Benth.",
  ## spp that naturally colonized
  "Amsinckia menziesii (Lehm.) A.Nelson & J.F.Macbr.",
  "Caulanthus lasiophyllus (Hook. & Arn.) Payson"
)

### Check if they worked
# name_backbone_checklist(name_list)

### Pull taxon keys from list
taxon_keys <- name_backbone_checklist(name_list) %>% 
  pull(usageKey)
```

Start with A. polycarpa
```{r}
### download filtered data for A. polycarpa
gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[2]),
                       ## remove geospatial issues
                       pred("hasGeospatialIssue", FALSE),
                       ## ensure coords
                       pred("hasCoordinate", TRUE),
                       ## remove "absent" occurences
                       pred("occurrenceStatus", "PRESENT"),
                       ## within US
                       pred("country", "US"),
                       ## within CA
                       pred("stateProvince", "California"),
                       ## uncertainty less than 5000m
                       #pred_lt("coordinateUncertaintyInMeters",5000),
                       ## output as CSV
                       format = "SIMPLE_CSV")

### check on download status
occ_download_wait(gbif_download)

### import GBIF data into env and filter using CoordinateCleaner pkg
p_arborea <- occ_download_get(gbif_download) %>%
  occ_download_import() %>% 
  ## set lowercase column names to work with CC
  setNames(tolower(names(.))) %>% 
  ## filter out duplicate points
  distinct(decimallongitude, decimallatitude, specieskey, datasetkey, .keep_all = TRUE)
```

Pull in CalFlora data for A. polycarpa, merge with GBIF data, and determine how many duplicates there are
```{r}
apoly_calf <- read_csv("data/a_polycarpa_calflora.csv") %>% 
  janitor::clean_names()

## subset of calflora data
apoly_calf_sub <- apoly_calf %>% 
  dplyr::select(c(latitude, longitude, date)) %>% 
  #mutate(source = "calflora") %>% 
  mutate(latitude = round(latitude, 4))

## subset of gbif data
apoly_gbif_sub <- a_poly %>% 
  dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
  rename(latitude = decimallatitude, 
         longitude = decimallongitude,  
         date = eventdate) %>% 
  #mutate(source = "gbif") %>% 
  mutate(latitude = round(latitude, 4))

apoly_gbif_sub$id <- as.character(apoly_gbif_sub$id)
apoly_gbif_sub$date <- as.Date(apoly_gbif_sub$date)


test <- inner_join(apoly_calf_sub, apoly_gbif_sub)

## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
## that came in both databases

apoly_gbif2 <- apoly_gbif_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

apoly_calf2 <- apoly_calf_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

test2 <- inner_join(apoly_gbif2, apoly_calf2)

## only 5 matching














## merge
apoly_merge <- rbind(apoly_calf_sub, apoly_gbif_sub)

## round lat
apoly_merge_sub <- apoly_merge %>% 
  mutate(latitude = round(latitude, 4)) %>% 
  group_by(date)


```
```{r}
parb_calf <- read_csv(here("data/p_arborea_calflora.csv")) %>% 
  janitor::clean_names()

## subset of calflora data
parb_calf_sub <- parb_calf %>% 
  dplyr::select(c(latitude, longitude, date)) %>% 
  #mutate(source = "calflora") %>% 
  mutate(latitude = round(latitude, 4))

## subset of gbif data
parb_gbif_sub <- p_arborea %>% 
  dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
  rename(latitude = decimallatitude, 
         longitude = decimallongitude,  
         date = eventdate) %>% 
  #mutate(source = "gbif") %>% 
  mutate(latitude = round(latitude, 4))

parb_gbif_sub$id <- as.character(apoly_gbif_sub$id)
parb_gbif_sub$date <- as.Date(parb_gbif_sub$date)


test <- inner_join(parb_calf_sub, parb_gbif_sub)

## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
## that came in both databases

parb_gbif2 <- parb_gbif_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

parb_calf2 <- parb_calf_sub %>% 
  mutate(latitude = round(latitude, 2),
         longitude = round(longitude, 2))

test2 <- inner_join(parb_gbif2, parb_calf2)

## only 5 matching
```








Eventually the SDM will be done with Dismo, but for now I'm starting with Wallace. Although GBIF datasets can be directly queried in the Wallace interface, you cannot customize the data filters as easily as using `rgbif` and `CoordinateCleaner`. As such, we'll have some temporary code that exports occurrence data locally that can be uploaded to Wallace interface.

```{r}
write_csv(a_poly, here("data/species_occurrence_GBIF/a_polycarpa.csv"))
```













Older version of cleaning data when manually downloading/reading in:

```{r}
### Fxn to read in and clean information from CSV
sp_filter <- function(filename) {
  ## read in data
  raw <- read_csv(here(paste0("data/species_occurrence_GBIF/raw/", filename))) %>% 
    janitor::clean_names()
  
  ## filter data
  filtered <- raw %>% 
    ## only keep obs in CA
    filter(state_province == "California") %>% 
    ## remove data with no coords
    drop_na(decimal_latitude, decimal_longitude) %>% 
    ## remove points with incorrect coords
    filter(!str_detect(issue, "COUNTRY_COORDINATE_MISMATCH"))
  
  return(filtered)
}

### test with a. poly
a_polycarpa <- sp_filter("A_polycarpa.csv")


blah <- read_csv(here("data/species_occurrence_GBIF/raw/A_polycarpa.csv"))
```





Convert folder of ASCII files to rasters. ASCII files have associated coordinates but usually not explicitly defined CRS when read in as a raster. Use this code to convert an entire file of .asc files to .tif files with appropriate CRS. BCM data uses NAD83 CA Albers (EPSG:3310)
```{r}
### Assign file path to global variable (selecting all .asc files)
fasc <- list.files(path = "data/" , pattern='\\.asc$', full=TRUE)
ftif <- gsub("\\.asc$", ".tif", fasc)

### Loop to read in .asc, add crs, then output .tif
for (i in 1:length(fasc)) {
    r <- rast(fasc[i])
    crs(r) <- "epsg: 3310"
    writeRaster(r, ftif[i])
}

## Test the new file
aet <- rast("data/aet.tif")

```

