---
title: "Filter species occurrence data"
author: "Nick McManus"
date: "2023-07-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)            ## always
library(here)                 ## reading/writing data
library(rgbif)                ## download GBIF data
library(CoordinateCleaner)    ## filter/clean GBIF data
library(terra)                ## rast pkg for quicker reprojecting
library(raster)               ## rast format that plays w/dismo
library(enmSdmX)              ## spatially thinning data
library(dismo)                ## generating background points
```

# Download/import Occurrence Data

This first section will read in, filter, and export species occurrence data from three different sources: GBIF, VegBank, and CalFlora. Both VegBank and CalFlora data are downloaded directly from their websites. GBIF data is imported and filtered using the `rgbif` and `CoordinateCleaner` packages, respectively. 


### CalFlora

This data was directly downloaded from the CalFlora website, found here:
https://www.calflora.org/entry/observ.html

A single CSV was downloaded for each species. We'll read them all in as one dataframe to more easily filter the data. 

*NOTE:* Creating a "low" and "high" filtered version of the data to see how much of a difference strictness over data sources plays. 
```{r}
path = here("data/occ/calflora//")

names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)

## List of accepted sources
sources <- paste(c("BLM",
                   "Bureau",
                   "USDA",
                   "DFW",
                   "USGS",
                   "Nature Conservancy",
                   "TNC",
                   "CNPS",
                   "Taylor",
                   "Hrusa"), collapse = "|")

## Loop through each spp to read in, filter, and export
for (i in 1:length(names)) {
  ## Read in data
  df <- read_csv(paste0(path, "download/", names[i], "_calflora.csv"))
  
  ## filter and keep coords
  df_filter <- df %>% 
    ## "LOW" filter criteria
    filter(`Location Quality` %in% c("high", "medium"),
           `Accuracy: Square Meters` <= 270,
           Date >= "1999-10-01") 
    ## "HIGH" filter criteria
    # filter(str_detect(.$Source, sources))
    
  ## export
  write_csv(df_filter, paste0(path, names[i], "_calflora_lowFilter.csv"))
}

```



### VegBank

This data was downloaded directly from the VegBank website, found here: http://vegbank.org/vegbank/forms/plot-query.jsp. 

Data for all plots of one spp were downloaded and zipped as a "batch". Information about the plots, contributors, and all the spp and % cover are written as different CSV files. 
Spatial information on the plots is limited; only one coordinate (assuming the center) is given for each plot. Sometimes the size of the plot is provided in m^2, sometimes it's reported qualitatively (e.g. "small" or "large"), and sometimes there is no information. Because presence data will be thinned in the next step, we'll assign one observation point for a plot under 270m^2 in size, regardless of % coverage. For plots where area is not reported, we'll err on the side of caution and only include one point as well. For plots over 270m^2..... TBD. The observation coordinate assigned will be the one reported for the plot in the VegBank database.

*NOTE:* we'll decide if this methodology should be changed later. 
```{r}
path = here("data/occ/vegbank//")

names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii"
  ## Not in vegbank
  # "c_lasiophyllus"
)

## Loop to read in, filter, and export for each spp
for (i in 1:length(names)) {
  
  ## Read in data
  plotData <- read_csv(paste0(path, names[i], "_noFilter/plot_env.csv"))
  
  ## filter and keep coords
  plotData_obs <- plotData %>% 
    mutate(date = lubridate::ymd(obsstartdate_vb)) %>% 
    filter(date >= "1999-10-01") %>% 
    ## Filter for under 270. Won't do for now
    # filter(area <= 270 | area == "null") %>% 
    dplyr::select(observation_id, authorplotcode_vb, 
                  project_id_name, date, latitude, longitude) %>% 
    mutate(spp = names[i])
  
  ## export
  write_csv(plotData_obs, paste0(path, names[i], "_vegbank.csv"))
}


```




### GBIF

This script uses the `rgbif` package to directly download species occurence data from GBIF. To use the `occ_download()` function, this requires setting your GBIF login details. Details on how to set this can be found at: 
https://docs.ropensci.org/rgbif/articles/gbif_credentials.html

```{r}
## Pull taxon keys from list of spp
taxon_keys <- name_backbone_checklist(c(
  ## shrubs
  "Atriplex polycarpa",
  "Peritoma arborea",
  ## forbs
  "Centromadia pungens",
  "Layia pentachaeta subsp. albida D.D.Keck", 
  "Phacelia ciliata Benth.",
  ## spp that naturally colonized
  "Amsinckia menziesii (Lehm.) A.Nelson & J.F.Macbr.",
  "Caulanthus lasiophyllus (Hook. & Arn.) Payson"
)) %>% 
  pull(usageKey)


names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)


## Loop through spp list and save each
for (i in 1:length(taxon_keys)) {
  ### download filtered data for A. polycarpa
  gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[i]),
                         ## remove geospatial issues
                         pred("hasGeospatialIssue", FALSE),
                         ## ensure coords
                         pred("hasCoordinate", TRUE),
                         ## remove "absent" occurences
                         pred("occurrenceStatus", "PRESENT"),
                         ## within US
                         pred("country", "US"),
                         ## within CA
                         pred("stateProvince", "California"),
                         ## uncertainty less than 5000m
                         #pred_lt("coordinateUncertaintyInMeters",5000),
                         ## output as CSV
                         format = "SIMPLE_CSV")
  
  ### check on download status
  occ_download_wait(gbif_download)
  
  ### import GBIF data into env and filter using CoordinateCleaner pkg
   species <- occ_download_get(gbif_download, 
                               path = here("data/occ/gbif/zips/"),
                               overwrite = TRUE) %>%
     occ_download_import() %>% 
     ## set lowercase column names to work with CC
     setNames(tolower(names(.))) %>% 
     ## filter out duplicate points
     distinct(decimallongitude, decimallatitude, 
              specieskey, datasetkey, .keep_all = TRUE) %>% 
     ## filter known uncertainty below 270 and keep NAs
     filter(coordinateuncertaintyinmeters < 270 | 
              is.na(coordinateuncertaintyinmeters)) %>% 
     ## known inaccurate default values
     filter(!coordinateuncertaintyinmeters %in% c(301,3036,999,9999)) %>% 
     ## remove herbaria/zoo locations
     cc_inst(lon = "decimallongitude", lat = "decimallatitude",
             buffer = 270, value = "clean", verbose = TRUE) %>% 
     ## remove ocean values
     cc_sea(lon = "decimallongitude", lat = "decimallatitude") %>% 
     ## remove points before 2000 wy
     filter(eventdate >= "1999-10-01")
   
   ## export file
   write_csv(species, paste0(here("data/occ/gbif//"),names[i],"_gbif.csv"))
   
} ## END LOOP
```


**OLD TEST CODE:**
Pull in CalFlora data for A. polycarpa, merge with GBIF data, and determine how many duplicates there are
```{r}
# apoly_calf <- read_csv("data/a_polycarpa_calflora.csv") %>% 
#   janitor::clean_names()
# 
# ## subset of calflora data
# apoly_calf_sub <- apoly_calf %>% 
#   dplyr::select(c(latitude, longitude, date)) %>% 
#   #mutate(source = "calflora") %>% 
#   mutate(latitude = round(latitude, 4))
# 
# ## subset of gbif data
# apoly_gbif_sub <- a_poly %>% 
#   dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
#   rename(latitude = decimallatitude, 
#          longitude = decimallongitude,  
#          date = eventdate) %>% 
#   #mutate(source = "gbif") %>% 
#   mutate(latitude = round(latitude, 4))
# 
# apoly_gbif_sub$id <- as.character(apoly_gbif_sub$id)
# apoly_gbif_sub$date <- as.Date(apoly_gbif_sub$date)
# 
# 
# test <- inner_join(apoly_calf_sub, apoly_gbif_sub)
# 
# ## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
# ## that came in both databases
# 
# apoly_gbif2 <- apoly_gbif_sub %>% 
#   mutate(latitude = round(latitude, 2),
#          longitude = round(longitude, 2))
# 
# apoly_calf2 <- apoly_calf_sub %>% 
#   mutate(latitude = round(latitude, 2),
#          longitude = round(longitude, 2))
# 
# test2 <- inner_join(apoly_gbif2, apoly_calf2)
# 
# ## only 5 matching
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# ## merge
# apoly_merge <- rbind(apoly_calf_sub, apoly_gbif_sub)
# 
# ## round lat
# apoly_merge_sub <- apoly_merge %>% 
#   mutate(latitude = round(latitude, 4)) %>% 
#   group_by(date)
# 
# 
# ```
# 
# ```{r}
# parb_calf <- read_csv(here("data/p_arborea_calflora.csv")) %>% 
#   janitor::clean_names()
# 
# ## subset of calflora data
# parb_calf_sub <- parb_calf %>% 
#   dplyr::select(c(latitude, longitude, date)) %>% 
#   #mutate(source = "calflora") %>% 
#   mutate(latitude = round(latitude, 4))
# 
# ## subset of gbif data
# parb_gbif_sub <- p_arborea %>% 
#   dplyr::select(c(decimallatitude, decimallongitude, eventdate)) %>% 
#   rename(latitude = decimallatitude, 
#          longitude = decimallongitude,  
#          date = eventdate) %>% 
#   #mutate(source = "gbif") %>% 
#   mutate(latitude = round(latitude, 4))
# 
# parb_gbif_sub$id <- as.character(apoly_gbif_sub$id)
# parb_gbif_sub$date <- as.Date(parb_gbif_sub$date)
# 
# 
# test <- inner_join(parb_calf_sub, parb_gbif_sub)
# 
# ## When rounded to 4 (10m), only two points match. These are both iNaturalist entiries
# ## that came in both databases
# 
# parb_gbif2 <- parb_gbif_sub %>% 
#   mutate(latitude = round(latitude, 2),
#          longitude = round(longitude, 2))
# 
# parb_calf2 <- parb_calf_sub %>% 
#   mutate(latitude = round(latitude, 2),
#          longitude = round(longitude, 2))
# 
# test2 <- inner_join(parb_gbif2, parb_calf2)
# 
# ## only 5 matching
```





# Merge and Spatially Thin

BCMv8 data are available at 270m resolution. To avoid biasing the model to oversampled regions, only one occurrence per 270m pixel will be used for extraction. We'll first combine occurrence data by species across all data sources, then thin and filter the data to be used in the SDM model. 
```{r}
## file paths 
path_gbif <- here("data/occ/gbif//")
path_calflora <- here("data/occ/calflora//")
path_vegbank <- here("data/occ/vegbank//")

## reference raster for thinning
rast <- rast(here('data/bcmv8/2000_2022/aet2020dec.tif')) %>% 
  ## match crs to spp occ data
  project(y = "WGS84")

## spp names
names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)


## Read in, merge, thin, and export for each spp on list
for (i in 1:length(names)) {
  ## GBIF ---------------------
  ### If no data for that species, doesn't contribute to merged file
  if (length(list.files(path_gbif, pattern = names[i])) == 0) {
    gbif = NULL
  } else {
    gbif <- read_csv(paste0(path_gbif, names[i], "_gbif.csv")) %>% 
      ## only select vars of interest
      dplyr::select(c(gbifid, decimallatitude, decimallongitude, eventdate)) %>% 
      ## consistent var names
      rename(id = gbifid,
             lat = decimallatitude,
             lon = decimallongitude,
             date = eventdate) %>%
             ## add source
      mutate(source = "gbif",
             ## remove time from date
             date = as.Date(date))
  }
  
  ## CalFlora -----------------
  if (length(list.files(path_calflora, pattern = names[i])) == 0) {
    calflora = NULL
  } else {
    calflora <- read_csv(paste0(path_calflora, names[i], "_calflora_lowFilter.csv")) %>% 
      dplyr::select(c(ID, Latitude, Longitude, Date)) %>% 
      rename(id = ID,
             lat = Latitude,
             lon = Longitude,
             date = Date) %>% 
      mutate(source = "calflora")
  }

  ## VegBank ------------------
  if (length(list.files(path_vegbank, pattern = names[i])) == 0) {
    vegbank = NULL
  } else {
    vegbank <- read_csv(paste0(path_vegbank, names[i], "_vegbank.csv")) %>% 
      dplyr::select(c(observation_id, latitude, longitude, date)) %>% 
      rename(id = observation_id, 
             lat = latitude,
             lon = longitude) %>% 
      mutate(source = "vegbank") %>% 
      ## check for incorred coord entries
      filter(lon < 0,
             lat > 0)
  }
           

  ## Merge and thin -----------
  combo <- rbind(gbif, calflora, vegbank)
  comboThin <- elimCellDuplicates(combo, rast, longLat = c("lon", "lat"))
  
  ## Export
  write_csv(comboThin, paste0(here("data/occ/combined_spp_occ//"), 
                              names[i],
                              "_lowFilter.csv"))
} ## END LOOP



```



```{r}

## Testing spThin -----------------------------------------
# thinTest <- thin(
#   loc.data = sppOcc,
#   lat.col = "decimallatitude",
#   long.col = "decimallongitude",
#   spec.col = "species",
#   thin.par = 0.27,
#   reps = 1,
#   locs.thinned.list.return = T,
#   write.files = F
#   # out.dir = here('data/species_occurrence_GBIF/')
# 
# )
# thinned <- as.data.frame(thinTest)
# #
# diff <- setdiff(sppThin, thinned)
# diff_vect <- diff %>%
#   vect(geom = c("Longitude", "Latitude"), crs = "WGS84")
# writeVector(diff_vect, here('data/diff'))
# #
# #
# enmTest_vect <- sppThin %>%
#   vect(geom = c("decimallongitude", "decimallatitude"), crs = "WGS84")
# 
# thinTest_vect <- thinned %>%
#   vect(geom = c("Longitude", "Latitude"), crs = "WGS84")


# writeVector(enmTest_vect, here('data/enmTest'))
# writeVector(thinTest_vect, here('data/thinTest'))

```



# Generate background points

The sample with enmSdmX is faster and compatible with terra, but the dismo one has the option of inputting occurrence data and NOT sampling those points... which may be a useful feature. For now using the Dismo fxn. 
**NOTE:** updated from 40pts per mo/yr to 150 (41400 total). 
```{r}
### Set up data inputs ----------------------------------------------
## Commented out code for presence data...
# pres <- read_csv(here('data/species_occurrence_GBIF/a_polycarpa_thinned.csv')) %>% 
#   dplyr::select(decimallatitude, decimallongitude) %>% 
#   rename(x = decimallongitude,
#          y = decimallatitude)
# ## Getting weird error in dismo::randomPoints
# ## Have to explicitly extablish values as dbl in a df
# pres$y = as.numeric(pres$y)
# pres$x = as.numeric(pres$x)
# pres <- as.data.frame(pres)

## Read in reference raster and make RasterLayer obj
rast <- rast(here('data/bcmv8/2000_2022/aet2020dec.tif')) %>% 
  project(y = "WGS84")
r <- raster(rast)

### Create random background points
## Random samples w/o replacement
backOcc <- randomPoints(mask = r, 
                        n = 41400,  
                        prob = FALSE)
backOcc <- as.data.frame(backOcc)

### assign 150 pts to each mo/yr
### first create df of mo/yr in wy format
dates <- data.frame(month = rep(1:12, each = 150),
                    year = rep(2000:2022, each = 1800))
dates <- rbind(tail(dates, 450), head(dates, -450))
dates[1:450, 2] = (dates[451,2] - 1)

backSamp <- cbind(backOcc, dates)

### export as csv
write_csv(backSamp, here('data/occ/background_points.csv'))
```





