---
title: "Environmental data prep"
author: Nick McManus
output: html_document
date: '2023-09-28'
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)    ## always
library(here)         ## reading/writing data
library(purrr)        ## run iterative fxns faster
library(sf)           ## vector objects
library(terra)        ## Better/faster GIS
library(raster)       ## GIS format required by Dismo

## Ended up not using these pkgs 
## but may be useful in the future
library(FedData)      ## Downloading SSURGO/NLCD data
library(tigris)       ## CA boundary shpfile
```


This script generates and preps the environmental data used in species distribution modeling. Outputs are read in for extraction in the `spp_occ_background.Rmd` as well as generating suitability prediction maps in the `kern_sdm.Rmd` markdown.


# BCMv8 Data

### Convert ASC to TIF
Publicly available BCMv8 files are in .asc format. ASCII files have associated coordinates but usually do not have an explicitly defined CRS when read in as a raster. Because the SDM will be run in a samples with data (SWD) format, .asc files are not necessary to run MaxEnt. Additionally, .tif have smaller file sizes than .asc, so conversion saves on data storage. The `asc_to_tif()` function converts an entire directory of .asc files to .tif with an inherent CRS. BCM data uses CA Albers NAD83 (EPSG:3310), so this is the default for the function.

BCMv8 data can be directly downloaded from USGS at:
https://www.sciencebase.gov/catalog/item/5f29c62d82cef313ed9edb39

```{r}
## Read in fxn
source(here('R/asc_to_tif.R'))

## Assign file path (selects all .asc files in directory)
filepath = here('data/bcmv8/2000_2022//')

## Run fxn
asc_to_tif(filepath, remove = TRUE)
```


### Quarterly rasters

#### Avg by water year
Using the `quarter_rast()` function, we'll generate rasters of either mean or cumulative values across the summer (Jun-Aug) or winter (Dec-Feb) quarter for each water year. These will then be used as additional environmental variables for modelings. 
```{r}
## Read in fxn
source(here("R/quarterly_rast.R"))

## Define fxn variables
pathIn = here("data/bcmv8/2000_2022//")
pathOut = here("data/bcmv8/quarterly_avgs//")
startYear = 2000
endYear = 2022

## Run for winter ppt
quarter_rast(var="ppt", quarter="winter", method="sum",
            startYear, endYear,
            pathIn, pathOut)

## Run for summer tmx
quarter_rast(var="tmx", quarter="summer", method = "mean",
            startYear, endYear,
            pathIn, pathOut)

```

#### Avg for entire time period
In addition to quarterly rasters by water year, we'll also need to make quarterly rasters averaged over the entire time period (2000-2022). These won't be used for generating the models, but for predicting habitat suitability with the model.  
```{r}
## Avg of winter precip ----------------------------------------------------
pptFiles <- list.files(
  ## read in the qtr avgs created in previous code chunk
  path = here("data/bcmv8/quarterly_avgs//"),
  ## only select winter ppt
  pattern = paste0("ppt", ".+", "winter_sum"),
  full.names = TRUE
)

## read in files as raster "stack"
pptStack <- terra::rast(c(pptFiles))
## find mean of all rasts
pptStack_avg <- terra::app(pptStack, fun = 'mean')
## export mean rast
writeRaster(pptStack_avg, here("data/bcmv8/monthly_avgs/ppt_winter_avg.tif"))


## Avg of summer tmx --------------------------------------------------------
tmxFiles <- list.files(
  path = here("data/bcmv8/quarterly_avgs//"),
  ## only select tmx files
  pattern = "tmx",
  full.names = TRUE
)

## read in as stack, find mean, then export
tmxStack <- terra::rast(c(tmxFiles))
tmxStack_avg <- terra::app(pptStack, fun = "mean")
writeRaster(tmxStack_avg, here("data/bcmv8/monthly_avgs/tmx_summer_avg.tif"))
```


### Avg monthly rasters:
To generate distribution probabilities by month, we'll also need to average monthly data over the entire time period (2000-2022 wy). First, we'll find the averages for the base BCM variables. Then, we'll have to use a slightly different function to find averaged temperature difference (tmax-tmin). 

Averages of base BCM variables:
```{r}
## df of variables to avg
vars <- data.frame(variable = rep(c("aet", "ppt", "tmn", "tmx", "cwd"), each = 12),
                   month = c('jan', 'feb', 'mar', 'apr', 'may', 'jun',
                             'jul', 'aug', 'sep', 'oct', 'nov', 'dec'))

## Fxn to avg -------------------------------------------------
  var_avg <- function(variable, month, pathIn, pathOut) {
    ## Read in all files for that var/mo
    files <- list.files(path = pathIn, 
                        ## only list those with matching yr/mo in name
                        pattern = paste0(variable, ".+", month),
                        full=TRUE)
    ## Stack and avg
    stack <- terra::rast(c(files))
    stack_avg <- terra::app(stack, fun = 'mean')
    
    ## Save
    writeRaster(stack_avg, 
                paste0(pathOut, variable, "_", month, "_avg.tif"), 
                overwrite = TRUE)
  }

## Run fxn w/pmap
pathIn = here("data/bcmv8/2000_2022//")
pathOut = here("data/bcmv8/monthly_avgs//")
purrr::pmap(.l=vars, .f=var_avg, pathIn, pathOut, .progress = TRUE)
```

Average of tdiff:
```{r}
vars <- data.frame(month = c('oct', 'nov', 'dec', 'jan', 'feb', 'mar', 
                             'apr', 'may', 'jun', 'jul', 'aug', 'sep'))

tdiff_avg <- function(month, pathIn, pathOut) {
  ## stack all tmx and tmn by month
  tmx.list <- list.files(path = pathIn,
                         pattern = paste0("tmx", ".+", month),
                         full.names = TRUE)
  tmn.list <- list.files(path = pathIn,
                         pattern = paste0("tmn", ".+", month),
                         full.names = TRUE)
  
  tmx <- terra::rast(c(tmx.list))
  tmn <- terra::rast(c(tmn.list))
  
  ## Find tdiff by month, then avg 
  tdiff <- tmx - tmn
  avg <- terra::app(tdiff, fun = "mean")
  
  ## Save
  writeRaster(avg, 
              paste0(pathOut, "tdiff_", month, "_avg.tif"), 
              overwrite = TRUE)
}

## Run fxn w/pmap
pathIn = here("data/bcmv8/2000_2022//")
pathOut = here("data/bcmv8/monthly_avgs//")
purrr::pmap(.l=vars, .f=tdiff_avg, pathIn, pathOut, .progress = TRUE)
```



### Future BCM data

Finally, to generate future species distribution probability maps, we'll have to wrangle projected environmental data. 

Testing out how to read in and work with the netCDF files of projected BCM data. 

```{r}
library(ncdf4)

## Read in w/modular format for looping --------------------
ncpath <- here("data/bcmv8/MIROC//")
ncname <- "MIROC5_rcp26_Monthly_tmx"  
ncfname <- paste0(ncpath, ncname, ".nc")

nc <- nc_open(ncfname)

## Get lon, lat, and time from file --------------------
lon <- ncvar_get(nc, "x")
nlon <- dim(lon) ##verify number of values

lat <- ncvar_get(nc, "y", verbose = F)
nlat <- dim(lat)

time <- ncvar_get(nc, "time")
head(time)
# [1]   0  31  61  92 123 151
## Looks like "monthly" data points, but we can
## double-check time units and number of time steps

tunits <- ncatt_get(nc,"time","units")
# tunits
# [1] "days since 2006-10-01"
nt <- dim(time)
# nt
# [1] 1116
## Confirmed unit of days, so with 1116 steps, each step a month,
## and start date of 2006, we can confirm it goes to 2099

## Get var of interest (monthly tmax)
tmx <- ncvar_get(nc, "MIROC5_rcp26_Monthly_tmx")
# Error: cannot allocate vector of size 64.9 Gb
```





# gNATSGO Data

*Why gNATSGO?:*
Due to the large number of species occurrences within NA pockets of California gSSURGO data, we'll be using the gridded National Soil Survey Geographic Database (gNATSGO) for extracting soil data. NATSGO data combines SSURGO and STATSGO, prioritizing SSURGO data where available and filling in missing areas with lower-level STATSGO data. gNATSGO also differs from gSSURGO in that mapunit level data is provided as a 10m resolution raster rather than a vector. To match the resolution of other environmental data and the coordinate precision with which species occurrence data is filtered, this raster will be aggregated to 270m resolution. 

*Note on methods:*
gNATSGO geodatabases contain both tabular and spatial data. Spatial attributes are only provided for mapunit level data, while tabular data is provided for mapunits, components, and horizons. Each mapunit contains one or more soil components, which in turn are vertically divided into soil horizons. Horizon-level data contains most of the soil characteristics we're interested in for modeling species distribution, such as pH, percent organic matter, and cation exchange capacity. In order to spatialize horizon-level data, we'll need to aggregate soil data and perform a series of joins. This is done by selecting a cut-off soil depth, then finding a weighted average of horizon-level data based on the thickness of each horizon. Once this horizon data is joined to a component, we can use the percent area of a component w/in a map unit to find the weighted average again. 

While tabular data from .gdb can be read into R, I have not yet found a method for directly reading in the mapunit raster. Currently, this raster is obtained by opening the .gdb in ArcGIS Pro and exporting the raster as a .tif file.

*Where to access:*
gNATSGO data by state can be directly downloaded as an ESRI geodatabase (.gdb) from the USDA's Natural Resources Conservation Service (NRCS):
https://nrcs.app.box.com/v/soils/folder/191785692827

More information about the database can be found on the NRCS' website:
https://www.nrcs.usda.gov/resources/data-and-reports/gridded-national-soil-survey-geographic-database-gnatsgo#download


### Aggregate soil data

Here we'll read in data from the locally saved gNATSGO .gdb. These data are inputs to the `natsgo_agg()` function, which outputs a .csv of aggregated horizon-level soil properties by mapunit.
```{r}
## database pathway
gdb = here("data/natsgo/gNATSGO_CA/gNATSGO_CA.gdb")

## Check the layers in the .gdb
# st_layers(gdb)

## Read in soil data using sf package to specify layer in database
## Comes in as df
horizon = st_read(gdb, layer = "chorizon")
component = st_read(gdb, layer = "component")
mapunit = st_read(gdb, layer = "mapunit")
## provides pre-aggregated data on drainage class,
## but not for other soil properties of interest
muagg = st_read(gdb, layer = "muaggatt")

## Read in and run fxn
source(here('R/natsgo_agg.R'))

natsgo_agg(horizon, component, mapunit, muagg,
           depth = 200, pathOut = here("data/natsgo//"))
```


### Rasterize soil data

To eventually run the monthly and future distribution probabilities, we need to have NATSGO soil data as rasters that can stack with the monthly BCM data. Now that the tabular soil data has been aggregated by mapunit, we'll reclassify the mapunit raster to generate one raster for each soil variable in the model (cec, ph, om, drainage class).

The raw gNATSGO raster is 10m resolution, so we'll first lower the resolution to 270m to match BCM data:
```{r}
## Read in soil data and raster
mu_r <- rast(here("data/natsgo/rasters/MapunitRaster_10m_CA_2023.tif"))

## Change resolution to 270m
mu270_r <- aggregate(mu_r, fact=27, 
                     ## mapunits are categorical
                     fun="modal")
## Save
writeRaster(mu270_r, here("data/natsgo/rasters/MapunitRaster_270m_CA_2023.tif"))
```

Now we'll create a new raster for each soil variable by reclassifying the mapunit raster and matching the extent of BCM data:
```{r}
## Read in data ----------------------------------------------------------
## mapunit raster
mu270_r <- rast(here("data/natsgo/rasters/MapunitRaster_270m_CA_2023.tif"))
## sample bcm raster
bcm_r<- rast(here("data/bcmv8/monthly_avgs/tmx_jan_avg.tif"))
## soil data
soil_df <- read_csv(here("data/natsgo/horizon_200cm_CA.csv")) %>% 
  ## turn drainage into factor, set levels
  mutate(drclass = factor(drclass, 
                             levels = c("Excessively drained",
                                        "Somewhat excessively drained",
                                        "Well drained",
                                        "Moderately well drained",
                                        "Somewhat poorly drained",
                                        "Poorly drained",
                                        "Very poorly drained"))) %>% 
  ## then reclass as numeric
  mutate(drclass = as.numeric(drclass))


## Create fxn to reclassify and match BCM raster extent -------------------
soil_rast <- function(var, type) {
  ## Select one variable at a time
  rcl <- soil_df %>% 
    dplyr::select(mukey, var) %>% 
    rbind(c(0, NA)) #make outside areas NA
  
  ## reclassify
  rcl_r <- classify(mu270_r, rcl)
  
  ## if continuous variable, resamp w/bilinear method
  if (type == "cont") {
    rcl_resamp_r <- rcl_r %>%
      project(y = crs(bcm_r), method = "bilinear") %>%
      resample(y = bcm_r, method = "bilinear")
  ## if categorical, use near method
  } else {
    rcl_resamp_r <- rcl_r %>%
      project(y = crs(bcm_r), method = "near") %>%
      resample(y = bcm_r, method = "near")
  }
  
  ## proper raster file name
  names(rcl_resamp_r) <- var
  
  ## Save
  writeRaster(rcl_resamp_r,
              paste0(
                here("data/natsgo/rasters/natsgo_"),
                var,
                "_270m_CA_2023.tif"
              ),
              overwrite = TRUE)
} ##End fxn

soil_rast(var = "drclass", type = "cat")

## Iterate fxn over all soil variables ------------------------------------
## var names must match columns in `soil_df`
soil_vars = data.frame(var = c("cec", "ph", "om", "drclass"),
                       type = c("cont", "cont", "cont", "cat"))
## run w/pmap
purrr::pmap(.l = soil_vars, 
            .f = soil_rast, 
            .progress = TRUE)
```
