---
title: "Species Distribution Model for Kern Natives"
author: "Nick McManus"
date: "2023-08-02"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)    ## always
library(here)         ## reading/writing data
library(purrr)        ## run iterative fxns faster
library(sf)           ## vector objects
library(terra)        ## Better/faster GIS
library(raster)       ## GIS format required by Dismo
library(dismo)        ## Maxent pkg
library(rJava)        ## Needed for dismo
library(lubridate)    ## Dates and progress bar
library(corrplot)     ## Correlation matrix
library(FedData)      ## Downloading SSURGO data
```


# Overview

This script extracts environmental data for each species occurrence (based on location and time). This data is then used to perform a species distribution model using Maxent with `dismo`.

*   Species occurrence data was pulled from GBIF (and maybe CalFlora?) and filtered to return reasonably accurate presence-only data in California. The script for this is in "src" directory.

*   Environmental data comes from the Basin Characterization Model version 8.0 (Flint et al., 2021) hosted by USGS. 


# Convert ASC to TIF

Publicly available BCMv8 files are in .asc format. ASCII files have associated coordinates but usually do not have an explicitly defined CRS when read in as a raster. Additionally, .tif have smaller file sizes than .asc, so conversion saves on data storage. The `asc_to_tif()` function converts an entire directory of .asc files to .tif files with appropriate CRS. BCM data uses NAD83 CA Albers (EPSG:3310), so this is the default for the function.
```{r}
# ## Read in fxn
# source(here('R/asc_to_tif.R'))
# 
# ## Assign file path (selects all .asc files in directory)
# filepath = here('data/bcmv8/2000_2023//')
# 
# ## Run fxn
# asc_to_tif(filepath)
```

# SSURGO data
We need to change the SSURGO data to match the resolution and spatial extent of the the BCM layers.
```{r}
## reference raster
bcm = rast(here("data/bcmv8/2000_2022/ppt2020jan.tif"))

## ssurgo rasters
ph = rast(here("data/ssurgo/ssurgo_ph.tif"))
som = rast(here("data/ssurgo/ssurgo_percent_som.tif"))

## change resolution from 90m to 270m (factor of 3) 
ph_270 <- aggregate(ph, fact=3, fun="mean", na.rm=TRUE) %>% 
  project(y = bcm)
## resample to ensure same extent
ph_resamp <- resample(ph_270, bcm, method="bilinear")
## export
writeRaster(ph_resamp, here("data/ssurgo/ssurgo_ph_270m.tif"))

## same with SOM
som_270 <- aggregate(som, fact=3, fun="mean", na.rm=TRUE) %>% 
  project(y=bcm)
som_resamp <- resample(som_270, bcm, method="bilinear")
writeRaster(som_resamp, here("data/ssurgo/ssurgo_som_270m.tif"))
```



### SSURGO import test
Testing out importing SSURGO data with `FedData` package. Starting out with only one map area. Involves importing data, then reducing number of horizons to 1:1 with components (right now by selecting for depth?), then taking weighted avg of components to generate 1:1 join with map unit, then should be able to spatially plot info for pH, SOM, and CEC???

**NOTE** Doesn't work with raster/vector of CA because the area is too big. Also tried reading in for all soil areas but it timed out before it could merge them all as one geopackage. Will instead download/extract separately, then read in from geopackage and merge. Not as nice as calling directly from `get_ssurgo()` output (which I left commented out for future reference).

The following soil survey areas were omitted for lacking horizon-level data: CA663, CA704, CA793, CA804, CA806
```{r}
## DF of soil survey areas in CA -----------------------------------------------
CA_surveyAreas = c("AZ649", "AZ656", "CA011", "CA013", "CA021", "CA031", 
                   "CA033", "CA041", "CA053", "CA055", "CA067", "CA069", 
                   "CA077", "CA087", "CA095", "CA097", "CA101", "CA113", 
                   "CA600", "CA601", "CA602", "CA603", "CA604", "CA605", 
                   "CA606", "CA607", "CA608", "CA609", "CA610", "CA612",
                   "CA614", "CA618", "CA619", "CA620", "CA624", "CA628", 
                   "CA630", "CA632", "CA637", "CA638", "CA641", "CA642", 
                   "CA644", "CA645", "CA646", "CA647", "CA648", "CA649", 
                   "CA651", "CA653", "CA654", "CA659", "CA660", "CA664", 
                   "CA665", "CA666", "CA667", "CA668", "CA669", "CA670",
                   "CA671", "CA672", "CA673", "CA674", "CA675", "CA676",
                   "CA677", "CA678", "CA679", "CA680", "CA681", "CA682", 
                   "CA683", "CA684", "CA685", "CA686", "CA687", "CA688", 
                   "CA689", "CA691", "CA692", "CA693", "CA694", "CA695",
                   "CA696", "CA697", "CA698", "CA699", "CA701", "CA702", 
                   "CA703", "CA707", "CA708", "CA709", "CA713", "CA719", 
                   "CA724", "CA729", "CA731", "CA732", "CA740", "CA750",
                   "CA760", "CA763", "CA772", "CA776", "CA777", "CA788", 
                   "CA789", "CA790", "CA792", "CA794", "CA795", "CA796", 
                   "CA802", "CA803", "CA805")

CA_df = data.frame(template = CA_surveyAreas,
                   label = CA_surveyAreas,
                   raw.dir = here("data/ssurgo/raw"),
                   extraction.dir = here("data/ssurgo/extracted"))


## Download and extract data locally -------------------------------------------
pmap(CA_df, get_ssurgo)


## Read in horizon, component, and mapunit data
## Merge across all survey areas -----------------------------------------------
## Horizon
chorizon = data.frame()
for (i in 1:nrow(CA_df)) {
  area = paste0(here("data/ssurgo/extracted//"), 
                CA_df$template[i], 
                "_ssurgo.gpkg")
  
  x <- st_read(area, layer = "chorizon")
  chorizon = rbind(chorizon, x)
}

## Component
component = data.frame()
for (i in 1:nrow(CA_df)) {
  area = paste0(here("data/ssurgo/extracted//"), 
                CA_df$template[i], 
                "_ssurgo.gpkg")
  
  x <- st_read(area, layer = "component")
  component = rbind(component, x)
}

## Mapunit
mapunit = data.frame()
for (i in 1:nrow(CA_df)) {
  area = paste0(here("data/ssurgo/extracted//"), 
                CA_df$template[i], 
                "_ssurgo.gpkg")
  
  x <- st_read(area, layer = "mapunit")
  mapunit = rbind(mapunit, x)
}

# ## **FOR REFERENCE**: If using fewer survey areas,
# ## use this code to automatically read in/merge dataset.
# ## Then filter/merge using same code below
# ssurgo = get_ssurgo(template = CA_surveyAreas, label = "CA")
# chorizon = ssurgo$tabular$chorizon
# component = ssurgo$tabular$component
# mapunit = ssurgo$tabular$mapunit
# ssurgo_sf = ssurgo$spatial

# ## Quick check on number of unique values to
# ## anticipate joining situation
# length(unique(component$cokey))
# [1] 91837
# length(unique(chorizon$cokey))
# [1] 42218
# ## Quite a few components w/o horizon level data
# length(unique(component$mukey))
# [1] 19126
# length(unique(mapunit$mukey))
# [1] 19126
# ## Matching # of mapunits (expected)


## Remove variables with all NAs
not_all_na <- function(x) any(!is.na(x)) #for data cleaning

chorizon <- chorizon %>% 
  select_if(not_all_na)
component <- component %>% 
  select_if(not_all_na)
mapunit <- mapunit %>% 
  select_if(not_all_na)


## Horizon level data ---------------------------------------------------------
## Finding total soil depth.... not sure why this is a step
depth = chorizon %>% 
  group_by(cokey) %>% 
  summarize(total_depth = max(hzdepb.r))

## Remove horizons that start below 30cm
chorizon30 = chorizon %>% 
  filter(hzdept.r < 30) %>% 
  droplevels()

## How many components still have more than 1 horizon
## after filtering in last step?
nrow(
  chorizon30 %>% 
    group_by(cokey) %>% 
    summarize(count = n()) %>% 
    filter(count > 1)
  )
# [1] 31000

## To deal w/this, we'll summarize characteristics of interest (e.g. pH, SOM, CEC)
## w/weighted mean of horizon thickness (above 30cm cutoff)
chorizon30_wmean = chorizon30 %>% 
  ## find thickness of each horizon
  mutate(thick = ifelse(hzdepb.r > 30, 30 - hzdept.r,
                        hzdepb.r - hzdept.r)) %>% 
  ## weighted mean of each variable by component
  group_by(cokey) %>% 
  summarize(om = round(weighted.mean(om.r, thick, na.rm = TRUE), 2),
            cec = round(weighted.mean(cec7.r, thick, na.rm = TRUE), 2),
            ph = round(weighted.mean(ph1to1h2o.r, thick),2)) %>% 
  ## join w/depth
  left_join(., depth, by = "cokey")


## Component level data --------------------------------------------------------
## filter for variables of interest
component = component %>% 
  dplyr::select(c(comppct.r, compname, slope.r, mukey, cokey))

## join with horizon data
component_horizon = left_join(component, chorizon30_wmean, by = "cokey")

## Find weighted average of variables based on % component in a mapunit
full_soil = component_horizon %>% 
  group_by(mukey) %>% 
  summarize(om = round(weighted.mean(om, comppct.r, na.rm = TRUE),2),
            cec = round(weighted.mean(cec, comppct.r, na.rm = TRUE), 2),
            ph = round(weighted.mean(ph, comppct.r, na.rm = TRUE),2),
            slope = round(weighted.mean(slope.r, comppct.r, na.rm = TRUE), 2)) %>% 
  ## join w/mapunit data
  left_join(., mapunit, by = "mukey")

## remove mapunit variables we don't care about (right now) and
## convert commas to _ in muname so its csv compatible 
full_soil <- full_soil %>% 
  mutate(muname = gsub(", ", "_", muname)) %>% 
  dplyr::select(!c(mukind, mapunitlfw.l:mapunitlfw.h)) %>% 
  ## convert mukey from dbl to char for spatial join step
  mutate(mukey = as.character(mukey))

## Save as intermediate CSV
write_csv(full_soil, here("data/ssurgo/horizon_CA.csv"))


## Merge spatial and map unit data ---------------------------------------------
## first read in/merge all spatial data (will take a while!)
ssurgo_sf = NULL
for (i in 1:nrow(CA_df)) {
  area = paste0(here("data/ssurgo/extracted//"), CA_df$template[i], "_ssurgo.gpkg")
  
  x <- st_read(area, layer = "geometry")
  ssurgo_sf = rbind(ssurgo_sf, x)
}

## Merge with our final soil data
ssurgo_sf <- ssurgo_sf %>% 
  janitor::clean_names() %>% 
  left_join(., full_soil, by = c("mukey", "musym"))

## Create separate sf for variables to be extracted
ssurgo_sf_filter <- ssurgo_sf %>% 
  dplyr::select(om, ph, cec)

## Save both as as .shp
st_write(ssurgo_sf_filter, here("data/ssurgo/ssurgo_ca_select.shp"))

## convert to vect first
## some variable names changed by st_write...
ssurgo_vect <- vect(ssurgo_sf)
writeVector(ssurgo_vect, here("data/ssurgo/ssurgo_ca_full.shp"))

```





# Quarterly rasters

Using the `quarter_rast()` function, we'll generate rasters of either mean or cumulative values across the summer (Jun-Aug) or winter (Dec-Feb) quarter for each water year. These will then be used as additional environmental variables for extraction. 
```{r}
## Read in fxn
source(here("R/quarterly_avg.R"))

## Define fxn variables
pathIn = here("data/bcmv8/2000_2022//")
pathOut = here("data/bcmv8/quarterly_avgs//")
startYear = 2000
endYear = 2022

## Run for winter ppt
quarter_rast(var="ppt", quarter="winter", method="mean",
            startYear, endYear,
            pathIn, pathOut)

quarter_rast(var="ppt", quarter="winter", method="sum",
            startYear, endYear,
            pathIn, pathOut)

## Run for summer tmx
quarter_rast(var="tmx", quarter="summer", method = "mean",
            startYear, endYear,
            pathIn, pathOut)

```



# Extract environmental data

Using BCMv8 monthly and quarterly rasters, we'll extract information for each species occurrence (and background point) within the given time frame. The output will be one CSV with environmental data for each point.  
```{r}
## Read in fxn
source(here('R/env_extract.R'))

## Define fxn variables
pathMonth = here("data/bcmv8/2000_2022//")
pathQuarter = here("data/bcmv8/quarterly_avgs//")
pathSoil = here("data/ssurgo//")
```


## Occurrence extraction:
```{r}
names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)

## Read in species occurrence df
sppOcc <- read_csv(paste0(here('data/occ/combined_spp_occ//'),
                          names[1],
                          "_lowFilter.csv"))

## Use fxn to extract env data at location of occurrences
occExtract_2000_2022 <- env_extract(startYear=2000, endYear=2022, 
                                    pathMonth, pathQuarter, pathSoil, sppOcc) %>% 
  rename(ph = ssurgo_ph,
         per_som = ssurgo_percent_som)

## Export
write_csv(occExtract_2000_2022, here('data/swd/occExtract_2000_2022.csv'))
```


## Background extraction:
First, we'll create a subset of the background points data that are located within 50km of the species occurrence points using the terra and sf packages. Then, we'll run the `env_extract()` fxn on this subset and export the results for use in a species-specific SDM.
```{r}
## Read in background df
backOcc <- read_csv(here('data/occ/background_points.csv')) %>% 
  janitor::clean_names()

## Vectorize sppOcc, find convex hull of pts, 
## then add buffer of 50km
sppZone <- convHull(vect(sppOcc, 
                         geom = c("lon", "lat"),
                         crs = "WGS84")) %>% 
  buffer(., width = 50000)

## Vectorize backOcc, crop it w/sppZone,
## then convert to sf obj
backOcc_crop_sf = st_as_sf(crop(vect(backOcc, 
                                     geom = c("x", "y"),
                                     crs = "WGS84"),
                                sppZone))

## Finally, turn cropped background pts
## back into df to use in env_extract() fxn.
## Need lon/lat as separate vars for fxn
backOcc_crop_df = cbind((data.frame(backOcc_crop_sf)),
                        (data.frame(st_coordinates(backOcc_crop_sf)))) %>%
  dplyr::select(!geometry)


## Use fxn to extract for background points
## NOTE: this will take a while to run
backExtract_2000_2022 <- env_extract(startYear=2000, endYear=2022,
                                     pathMonth, pathQuarter,
                                     backOcc_crop_df,
                                     lon = 'X', lat = 'Y')

## Export
write_csv(backExtract_2000_2022, here('data/swd/backExtract_2000_2022.csv'))
```



## Correlation

Assessing the correlation between environmental variables for SDM. Additional derived variables, such as mean temp and difference of temp, are generated. Then a Pearson correlation matrix is produced; based on the values, certain variables may be removed from the final SWD file.

*NOTE:* CHANGE THIS to be only for background pts 
```{r}
# testOcc <- read_csv(here('data/swd/occExtract_2000_2022.csv')) %>% 
#   mutate(tmean = (tmx+tmn)/2,
#          tdiff = tmx-tmn) %>% 
#   dplyr::select(aet:tdiff) %>% 
#   dplyr::select(!c(cwd, pet))
#
# occCor = cor(testOcc, method = "pearson", use = "complete.obs")
# corrplot(occCor, method = 'number', type = 'upper')


testBack <- read_csv(here('data/swd/backExtract_2000_2022.csv')) %>% 
  dplyr::select(aet:tmx_summer) %>% 
  mutate(tmean = (tmx+tmn)/2,
         tdiff = tmx-tmn) %>%
  dplyr::select(!c(cwd, pet, tmn, tmn))


cor <- cor(testBack, method = "pearson", use = "everything")
corrplot(cor, method = 'number', type = 'upper')

# summer <- testBack %>% 
#   dplyr::select(!ppt_winter) %>% 
#   filter(!is.na(tmx_summer))
# 
# summerCor = cor(summer, method = "pearson", use = "everything")
# corrplot(summerCor, method = 'number', type = 'upper')
# 
# winter <- testBack %>% 
#   dplyr::select(!tmx_summer) %>% 
#   filter(!is.na(ppt_winter))
# 
# winterCor = cor(winter, method = "pearson", use = "everything")
# corrplot(winterCor, method = 'number', type = 'upper')
```




# Maxent Models

Construct model using Samples with Data (SWD) method.


### Input data

Read in species occurrence and background data, then wrangle to put in proper format. The model will be run using two temperature predictors: mean temperature [(tmn + tmx)/2] and difference of temperature [tmx-tmn]. After these values are derived, both occurrence and background points will isolated by month and merged into one data frame per month .
```{r}
## Occurrence data ----------------------------------------------------------
swdOcc <- read_csv(here('data/swd/occExtract_2000_2022.csv')) %>% 
  ## Add tmean, tdiff, and presence values
  mutate(tdiff = round(tmx-tmn, 2),
         presence = 1) %>% 
  ## remove select variables
  dplyr::select(!c(gbifid, tmn, pet, cwd)) %>%  
  ## rename lon/lat cols
  rename(x = decimallongitude,
         y = decimallatitude)

## rearrange x and y to match background df
swdOcc <- swdOcc[, c(2, 1, 3:11)]


## Background Data ----------------------------------------------------------
swdBack <- read_csv(here('data/swd/backExtract_2000_2022.csv')) %>% 
  mutate(tdiff = round(tmx-tmn, 2),
         presence = 0) %>% 
  dplyr::select(!c(tmn, pet, cwd))


## Bind into one df and
## remove any rows in NAs
swd <- rbind(swdOcc, swdBack)
swd <- swd[complete.cases(swd), ]

write_csv(swd, here('data/swd/swd_2000_2022.csv'))
```

### Model fitting
```{r}
## Read in all points
swd <- read_csv(here('data/swd/swd_2000_2022.csv'))

## Select env predictors for model (presence + background)
x <- swd %>% 
  dplyr::select(aet:tdiff)

## Specify occurrence data
p <- swd %>% 
  dplyr::select(presence)

## Arguments/options to pass to Maxent()
args <- c('jackknife=TRUE', 
          'autofeature=TRUE', 
          'responsecurves=TRUE', 
          'linear=TRUE',
          'quadratic=TRUE',
          'threshold=FALSE',
          'hinge=FALSE',
          'replicates=5', 'replicatetype=crossvalidate')

## Path to save results
path <- here('data/maxent_outputs/fitting/')



## Run test model
testModel <- maxent(x, p, path=path, args=args, removeDuplicates = TRUE)
```


### Final model
```{r}
## Read in all points
swd <- read_csv(here('data/swd/swd_2000_2022.csv'))

## Select env predictors for model (presence + background)
x <- swd %>% 
  dplyr::select(aet:tdiff)

## Specify occurrence data
p <- swd %>% 
  dplyr::select(presence)

## Set Arguments/Options to Pass to Maxent
args <- c('jackknife=TRUE', 
          'autofeature=TRUE', 
          'responsecurves=TRUE', 
          'linear=TRUE',
          'quadratic=TRUE',
          'threshold=FALSE',
          'hinge=FALSE', 
          'maximumiterations=100000', 
          'writeplotdata=TRUE')

## Path to save results
path <- here('data/maxent_outputs/final/')

#Final Model Creation
testFinal <- maxent(x, p, path=path, args=args)


save(testFinal, file = here("data/maxent_outputs/final/Model.rData"))
```



## Predictions

Now, we want to project the Maxent model predictions for each month to determine how suitability changes during the year. 


### Average monthly environmental data:

First we need to create rasters representing the monthly average for each predictor variable used in the model.
```{r}
## Read in fxn
source(here('R/env_avg.R'))

## Run fxn (shows progress bar)
env_avg(var_names = c('aet', 'ppt', 'tmn', 'tmx'),
        pathIn = here('data/bcmv8/2000_2022//'),
        pathOut = here('data/bcmv8/monthly_avgs//'))
```


### Run by month:

Testing out prediction maps using only one year of monthly env data. 
NOTE; testing out how to make this fxn work with user defined name/amount of variables w/o having to manually change the function... will think more on later.
```{r}
variables = c('aet', 'ppt', 'tmn', 'tmx')
df <- as.data.frame(variables) %>% 
  mutate(varNum = seq(1:length(variables))) %>% 
  pivot_wider(names_from = varNum, names_prefix = "var", values_from = variables) %>% 
  slice(rep(1:n(), each = 12)) %>% 
  mutate(month = c('jan', 'feb', 'mar', 'apr', 'may', 'jun',
                   'jul', 'aug', 'sep', 'oct', 'nov', 'dec'))






## Read in fxn
source(here('R/pred_month.R'))

## Read in Maxent model
load(here('data/maxent_outputs/final/Model.rData'))

## Run fxn (shows progress bar)
pred_month(model = testFinal,
           pathIn = here('data/bcmv8/monthly_avgs//'),
           pathOut = here('data/maxent_outputs/predictions//'))
```



## Response Curves

```{r}

```


























