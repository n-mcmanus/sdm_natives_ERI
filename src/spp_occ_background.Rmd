---
title: "Species occurrence data and background points"
author: "Nick McManus"
date: "2023-07-07"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)            ## always
library(here)                 ## reading/writing data
library(rgbif)                ## download GBIF data
library(CoordinateCleaner)    ## filter/clean GBIF data
library(terra)                ## rast pkg for quicker reprojecting
library(raster)               ## rast format that plays w/dismo
library(enmSdmX)              ## spatially thinning data
library(dismo)                ## generating background points
library(tigris)               ## CA boundary shpfile
library(FedData)              ## NLCD data
```

This script filters and creates both the species occurrence and background point data files used for the species distribution model in the `kern_sdm` markdown.

# Download/import Occurrence Data
This first section will read in, filter, and export species occurrence data from three different sources: GBIF, VegBank, and CalFlora. Both VegBank and CalFlora data are downloaded directly from their websites. GBIF data is imported and filtered using the `rgbif` and `CoordinateCleaner` packages, respectively. 


### CalFlora
This data was directly downloaded from the CalFlora website, found here:
https://www.calflora.org/entry/observ.html

A single CSV was downloaded for each species. We'll read them all in as one dataframe to more easily filter the data. 

*NOTE:* Creating a "low" and "high" filtered version of the data to see how much of a difference strictness over data sources plays. 
```{r}
path = here("data/occ/calflora//")

names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)

# ## List of accepted sources
# sources <- paste(c("BLM",
#                    "Bureau",
#                    "USDA",
#                    "DFW",
#                    "USGS",
#                    "Nature Conservancy",
#                    "TNC",
#                    "CNPS",
#                    "Taylor",
#                    "Hrusa"), collapse = "|")

## Loop through each spp to read in, filter, and export
for (i in 1:length(names)) {
  ## Read in data
  df <- read_csv(paste0(path, "download/", names[i], "_calflora.csv"))
  
  ## filter and keep coords
  df_filter <- df %>% 
    ## "LOW" filter criteria
    filter(`Location Quality` %in% c("high", "medium"),
           `Accuracy: Square Meters` <= 72900,
           Date >= "1999-10-01") 
    ## "HIGH" filter criteria
    # filter(str_detect(.$Source, sources))
    
  ## export
  write_csv(df_filter, paste0(path, names[i], "_calflora_lowFilter.csv"))
}

```



### VegBank
This data was downloaded directly from the VegBank website, found here: http://vegbank.org/vegbank/forms/plot-query.jsp. 

Data for all plots of one spp were downloaded and zipped as a "batch". Information about the plots, contributors, and all the spp and % cover are written as different CSV files. 
Spatial information on the plots is limited; only one coordinate (assuming the center) is given for each plot. Sometimes the size of the plot is provided in m^2, sometimes it's reported qualitatively (e.g. "small" or "large"), and sometimes there is no information. Because presence data will be thinned in the next step, we'll assign one observation point for a plot under 270m^2 in size, regardless of % coverage. For plots where area is not reported, we'll err on the side of caution and only include one point as well. For plots over 270m^2..... TBD. The observation coordinate assigned will be the one reported for the plot in the VegBank database.

*NOTE:* we'll decide if this methodology should be changed later. 
```{r}
path = here("data/occ/vegbank//")

names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii"
  ## Not in vegbank
  # "c_lasiophyllus"
)

## Loop to read in, filter, and export for each spp
for (i in 1:length(names)) {
  
  ## Read in data
  plotData <- read_csv(paste0(path, names[i], "_noFilter/plot_env.csv"))
  
  ## filter and keep coords
  plotData_obs <- plotData %>% 
    mutate(date = lubridate::ymd(obsstartdate_vb)) %>% 
    filter(date >= "1999-10-01") %>% 
    ## Filter for under 270. Won't do for now
    # filter(area <= 270 | area == "null") %>% 
    dplyr::select(observation_id, authorplotcode_vb, 
                  project_id_name, date, latitude, longitude) %>% 
    mutate(spp = names[i])
  
  ## export
  write_csv(plotData_obs, paste0(path, names[i], "_vegbank.csv"))
}


```




### GBIF
This script uses the `rgbif` package to directly download species occurence data from GBIF. To use the `occ_download()` function, this requires setting your GBIF login details. Details on how to set this can be found at: 
https://docs.ropensci.org/rgbif/articles/gbif_credentials.html

```{r}
## Pull taxon keys from list of spp
taxon_keys <- name_backbone_checklist(c(
  ## shrubs
  "Atriplex polycarpa",
  "Peritoma arborea",
  ## forbs
  "Centromadia pungens",
  "Layia pentachaeta subsp. albida D.D.Keck", 
  "Phacelia ciliata Benth.",
  ## spp that naturally colonized
  "Amsinckia menziesii (Lehm.) A.Nelson & J.F.Macbr.",
  "Caulanthus lasiophyllus (Hook. & Arn.) Payson"
)) %>% 
  pull(usageKey)


names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)


## Loop through spp list and save each
for (i in 1:length(taxon_keys)) {
  ### download filtered data for A. polycarpa
  gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[i]),
                         ## remove geospatial issues
                         pred("hasGeospatialIssue", FALSE),
                         ## ensure coords
                         pred("hasCoordinate", TRUE),
                         ## remove "absent" occurences
                         pred("occurrenceStatus", "PRESENT"),
                         ## within US
                         pred("country", "US"),
                         ## within CA
                         pred("stateProvince", "California"),
                         ## uncertainty less than 5000m
                         #pred_lt("coordinateUncertaintyInMeters",5000),
                         ## output as CSV
                         format = "SIMPLE_CSV")
  
  ### check on download status
  occ_download_wait(gbif_download)
  
  ### import GBIF data into env and filter using CoordinateCleaner pkg
   species <- occ_download_get(gbif_download, 
                               path = here("data/occ/gbif/zips/"),
                               overwrite = TRUE) %>%
     occ_download_import() %>% 
     ## set lowercase column names to work with CC
     setNames(tolower(names(.))) %>% 
     ## filter out duplicate points
     distinct(decimallongitude, decimallatitude, 
              specieskey, datasetkey, .keep_all = TRUE) %>% 
     ## filter known uncertainty below 270 and keep NAs
     filter(coordinateuncertaintyinmeters < 270 | 
              is.na(coordinateuncertaintyinmeters)) %>% 
     ## known inaccurate default values
     filter(!coordinateuncertaintyinmeters %in% c(301,3036,999,9999)) %>% 
     ## remove herbaria/zoo locations
     cc_inst(lon = "decimallongitude", lat = "decimallatitude",
             buffer = 270, value = "clean", verbose = TRUE) %>% 
     ## remove ocean values
     cc_sea(lon = "decimallongitude", lat = "decimallatitude") %>% 
     ## remove points before 2000 wy
     filter(eventdate >= "1999-10-01")
   
   ## export file
   write_csv(species, paste0(here("data/occ/gbif//"),names[i],"_gbif.csv"))
   
} ## END LOOP
```

*   For *L. pentachaeta*, occurrence ID 2421771841 was manually removed due to likely incorrect coordinates from the institution


# Merge and Spatially Thin
BCMv8 data are available at 270m resolution. To avoid biasing the model to oversampled regions, only one occurrence per 270m pixel will be used for extraction. We'll first combine occurrence data by species across all data sources, then thin and filter the data to be used in the SDM model. 
```{r}
## file paths 
path_gbif <- here("data/occ/gbif//")
path_calflora <- here("data/occ/calflora//")
path_vegbank <- here("data/occ/vegbank//")

## reference raster for thinning
rast <- rast(here('data/bcmv8/2000_2022/aet2020dec.tif')) %>% 
  ## match crs to spp occ data
  project(y = "WGS84")

## spp names
names <- c(
  "a_polycarpa",
  "p_arborea",
  "c_pungens",
  "l_pentachaeta",
  "p_ciliata",
  "a_menziesii",
  "c_lasiophyllus"
)


## Read in, merge, thin, and export for each spp on list
for (i in 1:length(names)) {
  ## GBIF ---------------------
  ### If no data for that species, doesn't contribute to merged file
  if (length(list.files(path_gbif, pattern = names[i])) == 0) {
    gbif = NULL
  } else {
    gbif <- read_csv(paste0(path_gbif, names[i], "_gbif.csv")) %>% 
      ## only select vars of interest
      dplyr::select(c(gbifid, decimallatitude, decimallongitude, eventdate)) %>% 
      ## consistent var names
      rename(id = gbifid,
             lat = decimallatitude,
             lon = decimallongitude,
             date = eventdate) %>%
             ## add source
      mutate(source = "gbif",
             ## remove time from date
             date = as.Date(date))
  }
  
  ## CalFlora -----------------
  if (length(list.files(path_calflora, pattern = names[i])) == 0) {
    calflora = NULL
  } else {
    calflora <- read_csv(paste0(path_calflora, names[i], "_calflora_lowFilter.csv")) %>% 
      dplyr::select(c(ID, Latitude, Longitude, Date)) %>% 
      rename(id = ID,
             lat = Latitude,
             lon = Longitude,
             date = Date) %>% 
      mutate(source = "calflora")
  }

  ## VegBank ------------------
  if (length(list.files(path_vegbank, pattern = names[i])) == 0) {
    vegbank = NULL
  } else {
    vegbank <- read_csv(paste0(path_vegbank, names[i], "_vegbank.csv")) %>% 
      dplyr::select(c(observation_id, latitude, longitude, date)) %>% 
      rename(id = observation_id, 
             lat = latitude,
             lon = longitude) %>% 
      mutate(source = "vegbank") %>% 
      ## check for incorred coord entries
      filter(lon < 0,
             lat > 0)
  }
           

  ## Merge and thin -----------
  combo <- rbind(gbif, calflora, vegbank) %>% 
    mutate(year = lubridate::year(date),
           month = lubridate::month(date),
           .before = source)
  comboThin <- elimCellDuplicates(combo, rast, longLat = c("lon", "lat"))
  
  ## Export
  write_csv(comboThin, paste0(here("data/occ/combined_spp_occ//"), 
                              names[i],
                              "_lowFilter.csv"))
} ## END LOOP

```



# Generate background points
Finally, random background occurrence points will be generated across all of California using the `dismo` package and exported as a CSV file. 

**NOTE:** Updated from 40 to 150 pts per mo/yr (11,040 to 41,400 total). Rather than use points from all of CA to create SDM model, we'll only be using points within a buffered distance from occurrence points for a species. The total number was increased to ensure that there are enough within the sample of background points taken to train the model. This methodology may change later.

**NOTE (2):** Updated to again back to 37 pts per mo/yr (10,212 total), but only within are of species presences. 
- Testing no buffer, 5km, and 20km
- Realistically this buffer would be determined by distribution of plant seeds
```{r}
## read in spp occurrence points
sppOcc = read_csv(here("data/occ/combined_spp_occ/l_pentachaeta_lowFilter.csv"))

## Fxn to generate background points
back_pts <- function(sppOcc, buffer) {
    ## Vectorize sppOcc, find convex hull of pts, 
    ## then add buffer
    sppZone <- convHull(vect(sppOcc, 
                           geom = c("lon", "lat"),
                           crs = "WGS84")) %>% 
    buffer(., width = buffer)
    
    ## Reference raster; make RasterLayer obj
    rast <- rast(here('data/bcmv8/2000_2022/aet2020dec.tif')) %>% 
      project(y = "WGS84") %>% 
      crop(x=., y=sppZone, mask = TRUE)
    r <- raster(rast)
    
    ## Create random background points
    ## Random samples w/o replacement
    backOcc <- randomPoints(mask = r, 
                            n = 10212,  
                            prob = FALSE)
    backOcc <- as.data.frame(backOcc)
    
    ## Assign 37 pts to each mo/yr
    ## First create df of mo/yr in wy format, then bind to pts df
    dates <- data.frame(month = rep(1:12, each = 37),
                        year = rep(2000:2022, each = 444))
    dates <- rbind(tail(dates, 111), head(dates, -111))
    dates[1:111, 2] = (dates[112,2] - 1)
    
    backSamp <- cbind(backOcc, dates)
}

backOcc <- back_pts(sppOcc, buffer = 5000)

test = backOcc %>% 
  

### export
write_csv(backOcc, here('data/background/back_l_pentachaeta_lowfilter_5km.csv'))
```


### Remove heavy human land-use
Finding areas of urban or heavy farming to remove from potential background points. 

#### NLCD
```{r}
## Get CA boundary from 2021 census info
ca_shp <- tigris::states(resolution = "500k") %>% 
  janitor::clean_names() %>% 
  filter(name == "California")

## Download in NLCD data
years <- c("2001", "2004", "2006", "2008", 
           "2011", "2013", "2016", "2019", "2021")

for (i in 1:length(years)) {
  nlcd <- FedData::get_nlcd(template = ca_shp, label = "test",
                            year = years[i],
                            dataset = "landcover",
                            extraction.dir = here("data/"))
}
```

How many spp occ are on high/med intensity development? 
```{r}
## List only files with .tif extension (e.g. not .tif.aux)
rast_list <- grep(list.files(here("data/nlcd/rasters")), 
              pattern = ".tif.", invert = TRUE, value = TRUE)
rast_yrs <- as.numeric(regmatches(rast_list, regexpr("[0-9].*[0-9]", rast_list)))
rast_list_df <- data.frame(rast=rast_list,
                           year=rast_yrs)

names <- c("a_polycarpa", "a_menziesii", "p_ciliata", "l_pentachaeta")

## Loop through land-use extractions at occ point
## Output in tidy table
nlcd_extract <- data.frame()
plant_ext<- data.frame()
# p in 1:length(names)
for(p in 1:length(names)){
  x <- read_csv(paste0(here("data/background/occ/combined_spp_occ//"), 
                      names[p], "_lowFilter.csv"))
  plant_ext <- data.frame()
  
    for(i in 1:nrow(rast_list_df)){
      r <- rast(paste0(here("data/nlcd/rasters//"), rast_list_df$rast[i]))
      if(i != nrow(rast_list_df)) {
        x_range <- x %>% 
               filter(year >= rast_list_df$year[i] 
                      & year < rast_list_df$year[i+1])
      ##If filtering to last date, no next row to look at
      ##Just do 2021-present
      } else {
        x_range <- x %>% 
               filter(year >= rast_list_df$year[i])
      }
      
      ##If no entries in that time period, then skip to next 
      if(nrow(x_range) > 0) {
        v <- vect(x_range, geom = c("lon", "lat"), crs = "WGS84")
        ext <- extract(r, v, ID = FALSE) %>% 
          janitor::clean_names() %>% 
          cbind(x_range, .)
      } else {
        next
      }
      ##One df of all extracts for plant
      plant_ext <- rbind(plant_ext, ext)
      
    }##End inner loop
  
  ##Summarize data and add to one consistent df
  ##nlcd_extract main output
  plants <- plant_ext %>%
    group_by(class) %>%
    summarize(count = length(class)) %>%
    mutate(count_pct = round(count/sum(.$count)*100,2),
           species = names[p])

  nlcd_extract <- rbind(nlcd_extract, plants)
    
}##End outer loop

write_csv(nlcd_extract, here("data/nlcd/nlcd_occ_extract.csv"))

## Look at some classes
nlcd_summary <- nlcd_extract %>% 
  filter(class %in% c("Developed, Low Intensity",
                      "Developed, Medium Intensity",
                      "Developed High Intensity",
                      "Cultivated Crops")) 

## Results in table for sharing
library(kableExtra)
nlcd_summary %>% 
  filter(!species %in% c("a_polycarpa")) %>% 
  kbl() %>% 
  kable_styling(full_width=F, bootstrap_options = "striped")
```

Find all highly developed land between 2001-2021
```{r}
rast_list <- grep(list.files(here("data/nlcd/rasters"), full.names=FALSE), 
              pattern = ".tif.", invert = TRUE, value = TRUE)
rast_yrs <- as.numeric(regmatches(rast_list, regexpr("[0-9].*[0-9]", rast_list)))
rast_list <- grep(list.files(here("data/nlcd/rasters"), full.names=TRUE), 
              pattern = ".tif.", invert = TRUE, value = TRUE)
rcl <- c(0, 23, NA,
          24, 24, 1, 
          25, 95, NA)
rcl_m <- matrix(rcl, ncol = 3, byrow=TRUE)

for (i in 1:length(rast_list)){
  r <- rast(rast_list[i])
  r_rcl <- classify(r, rcl_m, right = NA) %>% 
    project(y = "epsg:5070")
  
  assign(paste0("rast_", rast_yrs[i]), r_rcl)
}

nlcd_dev <- rast_2001 + rast_2004 + rast_2006 + rast_2008 +
  rast_2011 + rast_2016 + rast_2019 + rast_2021

rcl_m <- matrix(c(), ncol=3, byrow=TRUE)

nlcd_dev_rcl <- classify(nlcd_dev, rcl_m, right = NA)


```

High dev doesn't change at all??? - No, checked in Q and something in loop is wrong. Need to revisit

#### FMMP
```{r}

```


