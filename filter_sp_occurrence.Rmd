---
title: "Filter species occurrence data"
author: "Nick McManus"
date: "2023-07-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)            ## always
library(here)                 ## reading/writing data
library(rgbif)                ## download GBIF data
library(CoordinateCleaner)    ## filter/clean GBIF data
```

## R Markdown

This script uses the `rgbif` package to directly download species occurence data from GBIF. To use the `occ_download()` function, this requires setting your GBIF login details. Details on how to set this can be found at: https://docs.ropensci.org/rgbif/articles/gbif_credentials.html

```{r}
### Create list of species names
name_list <- c(
  ## shrubs
  "Atriplex polycarpa",
  "Peritoma arborea",
  ## forbs
  "Centromadia pungens",
  "Layia pentachaeta subsp. albida D.D.Keck", 
  "Phacelia ciliata Benth.",
  ## spp that naturally colonized
  "Amsinckia menziesii (Lehm.) A.Nelson & J.F.Macbr.",
  "Caulanthus lasiophyllus (Hook. & Arn.) Payson"
)

### Check if they worked
# name_backbone_checklist(name_list)

### Pull taxon keys from list
taxon_keys <- name_backbone_checklist(name_list) %>% 
  pull(usageKey)
```

Start with A. polycarpa
```{r}
### download filtered data for A. polycarpa
gbif_download <- occ_download(pred_in("taxonKey", taxon_keys[1]),
                       ## remove geospatial issues
                       pred("hasGeospatialIssue", FALSE),
                       ## ensure coords
                       pred("hasCoordinate", TRUE),
                       ## remove "absent" occurences
                       pred("occurrenceStatus", "PRESENT"),
                       ## within US
                       pred("country", "US"),
                       ## within CA
                       pred("stateProvince", "California"),
                       ## uncertainty less than 5000m
                       #pred_lt("coordinateUncertaintyInMeters",5000),
                       ## output as CSV
                       format = "SIMPLE_CSV")

### check on download status
occ_download_wait(gbif_download)

### import GBIF data into env and filter using CoordinateCleaner pkg
a_poly <- occ_download_get(gbif_download) %>%
  occ_download_import() %>% 
  ## set lowercase column names to work with CC
  setNames(tolower(names(.))) %>% 
  ## filter out duplicate points
  distinct(decimallongitude, decimallatitude, specieskey, datasetkey, .keep_all = TRUE)
```

Eventually the SDM will be done with Dismo, but for now I'm starting with Wallace. Although GBIF datasets can be directly queried in the Wallace interface, you cannot customize the data filters as easily as using `rgbif` and `CoordinateCleaner`. As such, we'll have some temporary code that exports occurrence data locally that can be uploaded to Wallace interface.

```{r}
write_csv(a_poly, here("data/species_occurrence_GBIF/a_polycarpa.csv"))
```













Older version of cleaning data when manually downloading/reading in:

```{r}
### Fxn to read in and clean information from CSV
sp_filter <- function(filename) {
  ## read in data
  raw <- read_csv(here(paste0("data/species_occurrence_GBIF/raw/", filename))) %>% 
    janitor::clean_names()
  
  ## filter data
  filtered <- raw %>% 
    ## only keep obs in CA
    filter(state_province == "California") %>% 
    ## remove data with no coords
    drop_na(decimal_latitude, decimal_longitude) %>% 
    ## remove points with incorrect coords
    filter(!str_detect(issue, "COUNTRY_COORDINATE_MISMATCH"))
  
  return(filtered)
}

### test with a. poly
a_polycarpa <- sp_filter("A_polycarpa.csv")


blah <- read_csv(here("data/species_occurrence_GBIF/raw/A_polycarpa.csv"))
```

